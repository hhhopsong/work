{
 "cells": [
  {
   "cell_type": "code",
   "id": "39aa0c8d8394477a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:29:27.818418Z",
     "start_time": "2025-01-16T12:29:25.856621Z"
    }
   },
   "source": [
    "from adodbapi.ado_consts import directions\n",
    "from cartopy import crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import multiprocessing\n",
    "import sys\n",
    "import cartopy.feature as cfeature\n",
    "import cmaps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm as tq\n",
    "import xarray as xr\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter  # 专门提供经纬度的\n",
    "from cartopy.util import add_cyclic_point\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import ticker\n",
    "from matplotlib.pyplot import quiverkey\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from scipy.ndimage import filters\n",
    "\n",
    "from toolbar.significance_test import corr_test\n",
    "from toolbar.TN_WaveActivityFlux import TN_WAF_3D\n",
    "from toolbar.curved_quivers.modplot import *\n",
    "from toolbar.data_read import *\n",
    "\n",
    "\n",
    "def corr(time_series, data):\n",
    "    # 计算相关系数\n",
    "    # 将 data 重塑为二维：时间轴为第一个维度\n",
    "    reshaped_data = data.reshape(len(time_series), -1)\n",
    "\n",
    "    # 减去均值以标准化\n",
    "    time_series_mean = time_series - np.mean(time_series)\n",
    "    data_mean = reshaped_data - np.mean(reshaped_data, axis=0)\n",
    "\n",
    "    # 计算分子（协方差）\n",
    "    numerator = np.sum(data_mean * time_series_mean[:, np.newaxis], axis=0)\n",
    "\n",
    "    # 计算分母（标准差乘积）\n",
    "    denominator = np.sqrt(np.sum(data_mean ** 2, axis=0)) * np.sqrt(np.sum(time_series_mean ** 2))\n",
    "\n",
    "    # 相关系数\n",
    "    correlation = numerator / denominator\n",
    "\n",
    "    # 重塑为 (lat, lon)\n",
    "    correlation_map = correlation.reshape(data.shape[1:])\n",
    "    return correlation_map\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:35:34.660275Z",
     "start_time": "2025-01-16T12:29:48.941707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "K_type = xr.open_dataset(r\"D:/PyFile/p2/data/Time_type_AverFiltAll0.9%_0.3%_3.nc\")\n",
    "# z\n",
    "z_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 'z')\n",
    "z_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 'z')\n",
    "z = xr.concat([z_high, z_low], dim='level')\n",
    "# u\n",
    "u_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 'u')\n",
    "u_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 'u')\n",
    "u = xr.concat([u_high, u_low], dim='level')\n",
    "# v\n",
    "v_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 'v')\n",
    "v_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 'v')\n",
    "v = xr.concat([v_high, v_low], dim='level')\n",
    "# t\n",
    "t_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 't')\n",
    "t_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 't')\n",
    "t = xr.concat([t_high, t_low], dim='level')\n",
    "# pre\n",
    "pre = prec(\"E:/data/NOAA/PREC/precip.mon.anom.nc\", 1960, 2022)\n",
    "# sst\n",
    "sst = ersst(\"E:/data/NOAA/ERSSTv5/sst.mnmean.nc\", 1960, 2022)"
   ],
   "id": "58d3c8f6b6d02a0c",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.23 GiB for an array with shape (768, 3, 361, 720) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m u_low \u001B[38;5;241m=\u001B[39m era5_p(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1960\u001B[39m, \u001B[38;5;241m2022\u001B[39m, [\u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m500\u001B[39m, \u001B[38;5;241m850\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      8\u001B[0m u_high \u001B[38;5;241m=\u001B[39m era5_hp(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1960\u001B[39m, \u001B[38;5;241m2022\u001B[39m, [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m150\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m u \u001B[38;5;241m=\u001B[39m \u001B[43mxr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mu_high\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mu_low\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlevel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# v\u001B[39;00m\n\u001B[0;32m     11\u001B[0m v_low \u001B[38;5;241m=\u001B[39m era5_p(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1960\u001B[39m, \u001B[38;5;241m2022\u001B[39m, [\u001B[38;5;241m200\u001B[39m, \u001B[38;5;241m500\u001B[39m, \u001B[38;5;241m850\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\concat.py:252\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001B[0m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _dataarray_concat(\n\u001B[0;32m    241\u001B[0m         objs,\n\u001B[0;32m    242\u001B[0m         dim\u001B[38;5;241m=\u001B[39mdim,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    249\u001B[0m         combine_attrs\u001B[38;5;241m=\u001B[39mcombine_attrs,\n\u001B[0;32m    250\u001B[0m     )\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first_obj, Dataset):\n\u001B[1;32m--> 252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_dataset_concat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_vars\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoords\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompat\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpositions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpositions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcombine_attrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombine_attrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcan only concatenate xarray Dataset and DataArray \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjects, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(first_obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    267\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\concat.py:484\u001B[0m, in \u001B[0;36m_dataset_concat\u001B[1;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001B[0m\n\u001B[0;32m    481\u001B[0m \u001B[38;5;66;03m# Make sure we're working on a copy (we'll be loading variables)\u001B[39;00m\n\u001B[0;32m    482\u001B[0m datasets \u001B[38;5;241m=\u001B[39m [ds\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mfor\u001B[39;00m ds \u001B[38;5;129;01min\u001B[39;00m datasets]\n\u001B[0;32m    483\u001B[0m datasets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m--> 484\u001B[0m     \u001B[43malign\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdatasets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    485\u001B[0m )\n\u001B[0;32m    487\u001B[0m dim_coords, dims_sizes, coord_names, data_names, vars_order \u001B[38;5;241m=\u001B[39m _parse_datasets(\n\u001B[0;32m    488\u001B[0m     datasets\n\u001B[0;32m    489\u001B[0m )\n\u001B[0;32m    490\u001B[0m dim_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(dim_coords)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\alignment.py:787\u001B[0m, in \u001B[0;36malign\u001B[1;34m(join, copy, indexes, exclude, fill_value, *objects)\u001B[0m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    592\u001B[0m \u001B[38;5;124;03mGiven any number of Dataset and/or DataArray objects, returns new\u001B[39;00m\n\u001B[0;32m    593\u001B[0m \u001B[38;5;124;03mobjects with aligned indexes and dimension sizes.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    777\u001B[0m \n\u001B[0;32m    778\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    779\u001B[0m aligner \u001B[38;5;241m=\u001B[39m Aligner(\n\u001B[0;32m    780\u001B[0m     objects,\n\u001B[0;32m    781\u001B[0m     join\u001B[38;5;241m=\u001B[39mjoin,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    785\u001B[0m     fill_value\u001B[38;5;241m=\u001B[39mfill_value,\n\u001B[0;32m    786\u001B[0m )\n\u001B[1;32m--> 787\u001B[0m \u001B[43maligner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malign\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m aligner\u001B[38;5;241m.\u001B[39mresults\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\alignment.py:580\u001B[0m, in \u001B[0;36mAligner.align\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjects\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\alignment.py:555\u001B[0m, in \u001B[0;36mAligner.reindex_all\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreindex_all\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 555\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatching_indexes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    557\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatching_indexes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobjects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobjects_matching_indexes\u001B[49m\n\u001B[0;32m    559\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\alignment.py:556\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreindex_all\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\n\u001B[1;32m--> 556\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatching_indexes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    557\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj, matching_indexes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[0;32m    558\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjects, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjects_matching_indexes\n\u001B[0;32m    559\u001B[0m         )\n\u001B[0;32m    560\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\alignment.py:542\u001B[0m, in \u001B[0;36mAligner._reindex_one\u001B[1;34m(self, obj, matching_indexes)\u001B[0m\n\u001B[0;32m    539\u001B[0m new_indexes, new_variables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_indexes_and_vars(obj, matching_indexes)\n\u001B[0;32m    540\u001B[0m dim_pos_indexers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_dim_pos_indexers(matching_indexes)\n\u001B[1;32m--> 542\u001B[0m new_obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_callback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdim_pos_indexers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnew_variables\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnew_indexes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexclude_dims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexclude_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    551\u001B[0m new_obj\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mencoding\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m new_obj\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\dataset.py:2922\u001B[0m, in \u001B[0;36mDataset._reindex_callback\u001B[1;34m(self, aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\u001B[0m\n\u001B[0;32m   2916\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2917\u001B[0m     to_reindex \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   2918\u001B[0m         k: v\n\u001B[0;32m   2919\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariables\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   2920\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m variables \u001B[38;5;129;01mand\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m exclude_vars\n\u001B[0;32m   2921\u001B[0m     }\n\u001B[1;32m-> 2922\u001B[0m     reindexed_vars \u001B[38;5;241m=\u001B[39m \u001B[43malignment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreindex_variables\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mto_reindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdim_pos_indexers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maligner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2927\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maligner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2928\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2929\u001B[0m     new_variables\u001B[38;5;241m.\u001B[39mupdate(reindexed_vars)\n\u001B[0;32m   2930\u001B[0m     new_coord_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_coord_names \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mset\u001B[39m(new_indexes)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\alignment.py:75\u001B[0m, in \u001B[0;36mreindex_variables\u001B[1;34m(variables, dim_pos_indexers, copy, fill_value, sparse)\u001B[0m\n\u001B[0;32m     72\u001B[0m needs_masking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28many\u001B[39m(d \u001B[38;5;129;01min\u001B[39;00m masked_dims \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m var\u001B[38;5;241m.\u001B[39mdims)\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m needs_masking:\n\u001B[1;32m---> 75\u001B[0m     new_var \u001B[38;5;241m=\u001B[39m \u001B[43mvar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_with_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindxr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(is_full_slice(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m indxr):\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;66;03m# no reindexing necessary\u001B[39;00m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;66;03m# here we need to manually deal with copying data, since\u001B[39;00m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;66;03m# we neither created a new ndarray nor used fancy indexing\u001B[39;00m\n\u001B[0;32m     80\u001B[0m     new_var \u001B[38;5;241m=\u001B[39m var\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\variable.py:938\u001B[0m, in \u001B[0;36mVariable._getitem_with_mask\u001B[1;34m(self, key, fill_value)\u001B[0m\n\u001B[0;32m    934\u001B[0m     mask \u001B[38;5;241m=\u001B[39m indexing\u001B[38;5;241m.\u001B[39mcreate_mask(indexer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape, data)\n\u001B[0;32m    935\u001B[0m     \u001B[38;5;66;03m# we need to invert the mask in order to pass data first. This helps\u001B[39;00m\n\u001B[0;32m    936\u001B[0m     \u001B[38;5;66;03m# pint to choose the correct unit\u001B[39;00m\n\u001B[0;32m    937\u001B[0m     \u001B[38;5;66;03m# TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\u001B[39;00m\n\u001B[1;32m--> 938\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mduck_array_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogical_not\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    940\u001B[0m     \u001B[38;5;66;03m# array cannot be indexed along dimensions of size 0, so just\u001B[39;00m\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# build the mask directly instead.\u001B[39;00m\n\u001B[0;32m    942\u001B[0m     mask \u001B[38;5;241m=\u001B[39m indexing\u001B[38;5;241m.\u001B[39mcreate_mask(indexer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Meteorological\\lib\\site-packages\\xarray\\core\\duck_array_ops.py:305\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(condition, x, y)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Three argument where() with better dtype promotion rules.\"\"\"\u001B[39;00m\n\u001B[0;32m    304\u001B[0m xp \u001B[38;5;241m=\u001B[39m get_array_namespace(condition)\n\u001B[1;32m--> 305\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mxp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mas_shared_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 2.23 GiB for an array with shape (768, 3, 361, 720) and data type float32"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T06:50:37.154927Z",
     "start_time": "2025-01-15T06:50:34.389230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 春季超前相关\n",
    "Z = z.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "Z = Z.sel(time=Z['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat', 'lon')\n",
    "\n",
    "U = u.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "U = U.sel(time=U['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat', 'lon')\n",
    "\n",
    "V = v.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "V = V.sel(time=V['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat', 'lon')\n",
    "\n",
    "T = t.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "T = T.sel(time=T['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat', 'lon')\n",
    "\n",
    "Pre = pre.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "Pre = Pre.sel(time=Pre['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'lat', 'lon')\n",
    "\n",
    "Sst = sst.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "Sst = Sst.sel(time=Sst['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'lat', 'lon')\n",
    "\n",
    "corr_z = np.zeros((len(K_type['type']), len(Z['level']), len(Z['lat']), len(Z['lon'])))\n",
    "reg_z = np.zeros((len(K_type['type']), len(Z['level']), len(Z['lat']), len(Z['lon'])))\n",
    "corr_u = np.zeros((len(K_type['type']), len(U['level']), len(U['lat']), len(U['lon'])))\n",
    "corr_v = np.zeros((len(K_type['type']), len(V['level']), len(V['lat']), len(V['lon'])))\n",
    "corr_t = np.zeros((len(K_type['type']), len(T['level']), len(T['lat']), len(T['lon'])))\n",
    "corr_pre = np.zeros((len(K_type['type']), len(Pre['lat']), len(Pre['lon'])))\n",
    "corr_sst = np.zeros((len(K_type['type']), len(Sst['lat']), len(Sst['lon'])))\n",
    "\n",
    "for i in tq.trange(len(K_type['type'])):\n",
    "    time_series = K_type.sel(type=i + 1)['K'].data\n",
    "    time_series = time_series - np.polyval(np.polyfit(range(len(time_series)), time_series, 1), range(len(time_series)))\n",
    "    time_series = (time_series - np.mean(time_series)) / np.var(time_series)\n",
    "    for j in tq.trange(len(z['level'])):\n",
    "        lev = z['level'][j].data\n",
    "        corr_z[i, j] = corr(time_series, Z['z'].sel(level=lev).data)\n",
    "        corr_u[i, j] = corr(time_series, U['u'].sel(level=lev).data)\n",
    "        corr_v[i, j] = corr(time_series, V['v'].sel(level=lev).data)\n",
    "        corr_t[i, j] = corr(time_series, T['t'].sel(level=lev).data)\n",
    "        reg_z[i, j] = np.array([np.polyfit(time_series, f, 1)[0] for f in\n",
    "                                Z['z'].sel(level=lev).transpose('lat', 'lon', 'year').data.reshape(-1, len(time_series))]).reshape(\n",
    "                                    Z['z'].sel(level=lev).data.shape[1], Z['z'].sel(level=lev).data.shape[2])\n",
    "    corr_pre[i] = corr(time_series, Pre['pre'].data)\n",
    "    corr_sst[i] = corr(time_series, Sst['sst'].data)"
   ],
   "id": "c1f4a823a8f22e83",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T13:53:18.118901Z",
     "start_time": "2025-01-14T13:53:17.623292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "corr_nc = xr.Dataset(\n",
    "    {\n",
    "        'corr_z': (['type', 'level', 'lat', 'lon'], corr_z),\n",
    "        'corr_u': (['type', 'level', 'lat', 'lon'], corr_u),\n",
    "        'corr_v': (['type', 'level', 'lat', 'lon'], corr_v),\n",
    "        'corr_t': (['type', 'level', 'lat', 'lon'], corr_t),\n",
    "        'corr_pre': (['type', 'prelat', 'prelon'], corr_pre),\n",
    "        'corr_sst': (['type', 'sstlat', 'sstlon'], corr_sst),\n",
    "        'reg_z': (['type', 'level', 'lat', 'lon'], reg_z)\n",
    "    },\n",
    "    coords={\n",
    "        'type': K_type['type'].data,\n",
    "        'level': Z['level'].data,\n",
    "        'lat': Z['lat'].data,\n",
    "        'lon': Z['lon'].data,\n",
    "        'prelat': Pre['lat'].data,\n",
    "        'prelon': Pre['lon'].data,\n",
    "        'sstlat': Sst['lat'].data,\n",
    "        'sstlon': Sst['lon'].data\n",
    "    }\n",
    ")\n",
    "corr_nc.to_netcdf(r\"D:/PyFile/p2/data/corr_MAM.nc\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T06:57:13.137140Z",
     "start_time": "2025-01-15T06:53:36.422507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 冬季超前相关\n",
    "def lead_crossyear(data):\n",
    "    Z = data.sel(time=slice('1960-03-01', '2022-11-30'))\n",
    "    # 提取时间的年份和月份\n",
    "    month = Z['time.month']\n",
    "    Z_winter = Z.sel(time=month.isin([12, 1, 2]))\n",
    "    year = Z_winter['time.year']\n",
    "    month = Z_winter['time.month']\n",
    "    # 创建一个新的年份坐标，对于12月，年份加1\n",
    "    winter_year = year.where(month != 12, year + 1)\n",
    "    Z_winter_avg = Z_winter.groupby(winter_year).mean('time')\n",
    "    return Z_winter_avg\n",
    "\n",
    "Z = lead_crossyear(z).transpose('year', 'level', 'lat', 'lon')\n",
    "U = lead_crossyear(u).transpose('year', 'level', 'lat', 'lon')\n",
    "V = lead_crossyear(v).transpose('year', 'level', 'lat', 'lon')\n",
    "T = lead_crossyear(t).transpose('year', 'level', 'lat', 'lon')\n",
    "Pre = lead_crossyear(pre).transpose('year', 'lat', 'lon')\n",
    "Sst = lead_crossyear(sst).transpose('year', 'lat', 'lon')\n",
    "\n",
    "corr_z = np.zeros((len(K_type['type']), len(Z['level']), len(Z['lat']), len(Z['lon'])))\n",
    "reg_z = np.zeros((len(K_type['type']), len(Z['level']), len(Z['lat']), len(Z['lon'])))\n",
    "corr_u = np.zeros((len(K_type['type']), len(U['level']), len(U['lat']), len(U['lon'])))\n",
    "corr_v = np.zeros((len(K_type['type']), len(V['level']), len(V['lat']), len(V['lon'])))\n",
    "corr_t = np.zeros((len(K_type['type']), len(T['level']), len(T['lat']), len(T['lon'])))\n",
    "corr_pre = np.zeros((len(K_type['type']), len(Pre['lat']), len(Pre['lon'])))\n",
    "corr_sst = np.zeros((len(K_type['type']), len(Sst['lat']), len(Sst['lon'])))\n",
    "\n",
    "for i in tq.trange(len(K_type['type'])):\n",
    "    time_series = K_type.sel(type=i + 1)['K'].data\n",
    "    time_series = time_series - np.polyval(np.polyfit(range(len(time_series)), time_series, 1), range(len(time_series)))\n",
    "    time_series = (time_series - np.mean(time_series)) / np.var(time_series)\n",
    "    for j in tq.trange(len(z['level'])):\n",
    "        lev = z['level'][j].data\n",
    "        corr_z[i, j] = corr(time_series, Z['z'].sel(level=lev).data)\n",
    "        corr_u[i, j] = corr(time_series, U['u'].sel(level=lev).data)\n",
    "        corr_v[i, j] = corr(time_series, V['v'].sel(level=lev).data)\n",
    "        corr_t[i, j] = corr(time_series, T['t'].sel(level=lev).data)\n",
    "        reg_z[i, j] = np.array([np.polyfit(time_series, f, 1)[0] for f in\n",
    "                                Z['z'].sel(level=lev).transpose('lat', 'lon', 'year').data.reshape(-1,\n",
    "                                                                                                   len(time_series))]).reshape(\n",
    "            Z['z'].sel(level=lev).data.shape[1], Z['z'].sel(level=lev).data.shape[2])\n",
    "    corr_pre[i] = corr(time_series, Pre['pre'].data)\n",
    "    corr_sst[i] = corr(time_series, Sst['sst'].data)\n",
    "\n",
    "corr_nc = xr.Dataset(\n",
    "    {\n",
    "        'corr_z': (['type', 'level', 'lat', 'lon'], corr_z),\n",
    "        'corr_u': (['type', 'level', 'lat', 'lon'], corr_u),\n",
    "        'corr_v': (['type', 'level', 'lat', 'lon'], corr_v),\n",
    "        'corr_t': (['type', 'level', 'lat', 'lon'], corr_t),\n",
    "        'corr_pre': (['type', 'prelat', 'prelon'], corr_pre),\n",
    "        'corr_sst': (['type', 'sstlat', 'sstlon'], corr_sst),\n",
    "        'reg_z': (['type', 'level', 'lat', 'lon'], reg_z)\n",
    "    },\n",
    "    coords={\n",
    "        'type': K_type['type'].data,\n",
    "        'level': Z['level'].data,\n",
    "        'lat': Z['lat'].data,\n",
    "        'lon': Z['lon'].data,\n",
    "        'prelat': Pre['lat'].data,\n",
    "        'prelon': Pre['lon'].data,\n",
    "        'sstlat': Sst['lat'].data,\n",
    "        'sstlon': Sst['lon'].data\n",
    "    }\n",
    ")\n",
    "corr_nc.to_netcdf(r\"D:/PyFile/p2/data/corr_DJF.nc\")\n"
   ],
   "id": "1400006a2a0e3d6f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:12<00:48, 12.11s/it]\u001B[A\n",
      " 40%|████      | 2/5 [00:23<00:35, 11.88s/it]\u001B[A\n",
      " 60%|██████    | 3/5 [00:35<00:23, 11.73s/it]\u001B[A\n",
      " 80%|████████  | 4/5 [00:46<00:11, 11.65s/it]\u001B[A\n",
      "100%|██████████| 5/5 [00:58<00:00, 11.73s/it]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:59<01:58, 59.19s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:11<00:46, 11.51s/it]\u001B[A\n",
      " 40%|████      | 2/5 [00:23<00:34, 11.55s/it]\u001B[A\n",
      " 60%|██████    | 3/5 [00:34<00:23, 11.54s/it]\u001B[A\n",
      " 80%|████████  | 4/5 [00:46<00:11, 11.63s/it]\u001B[A\n",
      "100%|██████████| 5/5 [00:57<00:00, 11.59s/it]\u001B[A\n",
      " 67%|██████▋   | 2/3 [01:57<00:58, 58.49s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:11<00:46, 11.56s/it]\u001B[A\n",
      " 40%|████      | 2/5 [00:23<00:34, 11.62s/it]\u001B[A\n",
      " 60%|██████    | 3/5 [00:34<00:23, 11.60s/it]\u001B[A\n",
      " 80%|████████  | 4/5 [00:46<00:11, 11.68s/it]\u001B[A\n",
      "100%|██████████| 5/5 [00:58<00:00, 11.64s/it]\u001B[A\n",
      "100%|██████████| 3/3 [02:55<00:00, 58.47s/it]\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
