{
 "cells": [
  {
   "cell_type": "code",
   "id": "58d3c8f6b6d02a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T09:53:40.826091Z",
     "start_time": "2025-01-14T09:41:57.446381Z"
    }
   },
   "source": [
    "from adodbapi.ado_consts import directions\n",
    "from cartopy import crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import multiprocessing\n",
    "import sys\n",
    "import cartopy.feature as cfeature\n",
    "import cmaps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm as tq\n",
    "import xarray as xr\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter  # 专门提供经纬度的\n",
    "from cartopy.util import add_cyclic_point\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import ticker\n",
    "from matplotlib.pyplot import quiverkey\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from scipy.ndimage import filters\n",
    "\n",
    "from toolbar.significance_test import corr_test\n",
    "from toolbar.TN_WaveActivityFlux import TN_WAF_3D\n",
    "from toolbar.curved_quivers.modplot import *\n",
    "from toolbar.data_read import *\n",
    "\n",
    "\n",
    "def corr(time_series, data):\n",
    "    # 计算相关系数\n",
    "    # 将 data 重塑为二维：时间轴为第一个维度\n",
    "    reshaped_data = data.reshape(len(time_series), -1)\n",
    "\n",
    "    # 减去均值以标准化\n",
    "    time_series_mean = time_series - np.mean(time_series)\n",
    "    data_mean = reshaped_data - np.mean(reshaped_data, axis=0)\n",
    "\n",
    "    # 计算分子（协方差）\n",
    "    numerator = np.sum(data_mean * time_series_mean[:, np.newaxis], axis=0)\n",
    "\n",
    "    # 计算分母（标准差乘积）\n",
    "    denominator = np.sqrt(np.sum(data_mean ** 2, axis=0)) * np.sqrt(np.sum(time_series_mean ** 2))\n",
    "\n",
    "    # 相关系数\n",
    "    correlation = numerator / denominator\n",
    "\n",
    "    # 重塑为 (lat, lon)\n",
    "    correlation_map = correlation.reshape(data.shape[1:])\n",
    "    return correlation_map\n",
    "\n",
    "\n",
    "K_type = xr.open_dataset(r\"D:/PyFile/p2/data/Time_type_AverFiltAll0.9%_0.3%_3.nc\")\n",
    "# z\n",
    "z_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 'z')\n",
    "z_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 'z')\n",
    "z = xr.concat([z_high, z_low], dim='level')\n",
    "# u\n",
    "u_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 'u')\n",
    "u_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 'u')\n",
    "u = xr.concat([u_high, u_low], dim='level')\n",
    "# v\n",
    "v_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 'v')\n",
    "v_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 'v')\n",
    "v = xr.concat([v_high, v_low], dim='level')\n",
    "# t\n",
    "t_low = era5_p(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev.nc\", 1960, 2022, [200, 500, 850], 't')\n",
    "t_high = era5_hp(\"E:/data/ERA5/ERA5_pressLev/era5_pressLev_high.nc\", 1960, 2022, [100, 150], 't')\n",
    "t = xr.concat([t_high, t_low], dim='level')\n",
    "# pre\n",
    "pre = prec(\"E:/data/NOAA/PREC/precip.mon.anom.nc\", 1960, 2022)\n",
    "# sst\n",
    "sst = ersst(\"E:/data/NOAA/ERSSTv5/sst.mnmean.nc\", 1960, 2022)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-14T10:07:19.993160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 春季超前相关\n",
    "Z = z.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "Z = Z.sel(time=Z['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat',\n",
    "                                                                                            'lon')\n",
    "U = u.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "U = U.sel(time=U['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat',\n",
    "                                                                                            'lon')\n",
    "V = v.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "V = V.sel(time=V['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat',\n",
    "                                                                                            'lon')\n",
    "T = t.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "T = T.sel(time=T['time.month'].isin([3, 4, 5])).groupby('time.year').mean('time').transpose('year', 'level', 'lat', 'lon')\n",
    "\n",
    "Pre = pre.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "Pre = Pre.sel(time=Pre['time.month'].isin([3, 4, 5])).groupby(\n",
    "    'time.year').mean('time').transpose('year', 'lat', 'lon')\n",
    "Sst = sst.sel(time=slice('1961-01-01', '2022-12-31'))\n",
    "Sst = Sst.sel(time=Sst['time.month'].isin([3, 4, 5])).groupby(\n",
    "    'time.year').mean('time').transpose('year', 'lat', 'lon')\n",
    "corr_z = np.zeros((len(K_type['type']), len(Z['level']), len(Z['lat']), len(Z['lon'])))\n",
    "reg_z = np.zeros((len(K_type['type']), len(Z['level']), len(Z['lat']), len(Z['lon'])))\n",
    "corr_u = np.zeros((len(K_type['type']), len(U['level']), len(U['lat']), len(U['lon'])))\n",
    "corr_v = np.zeros((len(K_type['type']), len(V['level']), len(V['lat']), len(V['lon'])))\n",
    "corr_t = np.zeros((len(K_type['type']), len(T['level']), len(T['lat']), len(T['lon'])))\n",
    "corr_pre = np.zeros((len(K_type['type']), len(Pre['lat']), len(Pre['lon'])))\n",
    "corr_sst = np.zeros((len(K_type['type']), len(Sst['lat']), len(Sst['lon'])))\n",
    "\n",
    "for i in tq.trange(len(K_type['type'])):\n",
    "    time_series = K_type.sel(type=i + 1)['K'].data\n",
    "    time_series = time_series - np.polyval(np.polyfit(range(len(time_series)), time_series, 1), range(len(time_series)))\n",
    "    time_series = (time_series - np.mean(time_series)) / np.var(time_series)\n",
    "    for j in tq.trange(len(z['level'])):\n",
    "        lev = z['level'][j].data\n",
    "        corr_z[i, j] = corr(time_series, Z['z'].sel(level=lev).data)\n",
    "        corr_u[i, j] = corr(time_series, U['u'].sel(level=lev).data)\n",
    "        corr_v[i, j] = corr(time_series, V['v'].sel(level=lev).data)\n",
    "        corr_t[i, j] = corr(time_series, T['t'].sel(level=lev).data)\n",
    "        reg_z[i, j] = np.array([np.polyfit(time_series, f, 1)[0] for f in\n",
    "                                Z['z'].sel(level=lev).transpose('lat', 'lon', 'year').data.reshape(-1,\n",
    "                                                                                                   len(time_series))]).reshape(\n",
    "            Z['z'].sel(level=lev).data.shape[1], Z['z'].sel(level=lev).data.shape[2])\n",
    "    corr_pre[i] = corr(time_series, Pre['pre'].data)\n",
    "    corr_sst[i] = corr(time_series, Sst['sst'].data)\n",
    "\n",
    "corr_nc = xr.Dataset(\n",
    "    {\n",
    "        'corr_z': (['type', 'level', 'lat', 'lon'], corr_z),\n",
    "        'corr_u': (['type', 'level', 'lat', 'lon'], corr_u),\n",
    "        'corr_v': (['type', 'level', 'lat', 'lon'], corr_v),\n",
    "        'corr_t': (['type', 'level', 'lat', 'lon'], corr_t),\n",
    "        'corr_pre': (['type', 'prelat', 'prelon'], corr_pre),\n",
    "        'corr_sst': (['type', 'sstlat', 'sstlon'], corr_sst),\n",
    "        'reg_z': (['type', 'level', 'lat', 'lon'], reg_z)\n",
    "    },\n",
    "    coords={\n",
    "        'type': K_type['type'],\n",
    "        'level': Z['level'],\n",
    "        'lat': Z['lat'],\n",
    "        'lon': Z['lon'],\n",
    "        'prelat': Pre['lat'],\n",
    "        'prelon': Pre['lon'],\n",
    "        'sstlat': Sst['lat'],\n",
    "        'sstlon': Sst['lon']\n",
    "    }\n",
    ")\n",
    "corr_nc.to_netcdf(r\"D:/PyFile/p2/data/corr_MAM.nc\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:11<00:46, 11.74s/it]\u001B[A\n",
      " 40%|████      | 2/5 [00:23<00:35, 11.72s/it]\u001B[A\n",
      " 60%|██████    | 3/5 [00:35<00:23, 11.74s/it]\u001B[A\n",
      " 80%|████████  | 4/5 [00:47<00:11, 11.82s/it]\u001B[A\n",
      "100%|██████████| 5/5 [00:58<00:00, 11.80s/it]\u001B[A\n",
      " 33%|███▎      | 1/3 [00:59<01:58, 59.33s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
